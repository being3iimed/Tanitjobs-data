{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/being3iimed/Tanitjobs-data/blob/main/FScrappingTanitjobs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K7AZZNbw76Rg"
      },
      "source": [
        "# **Scrapping Stage**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_pXWsEMG0J61"
      },
      "source": [
        "Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XjOgHhxWzrkI"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import time\n",
        "import csv\n",
        "import re\n",
        "from bs4 import BeautifulSoup as bs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMbvS8t90DSj"
      },
      "source": [
        "Writing to csv file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9X45U3WQzotY"
      },
      "outputs": [],
      "source": [
        "class Writer :\n",
        "  #todo\n",
        "  def __init__(self, filename) :\n",
        "    self.filename = str(filename)\n",
        "\n",
        "  def write_to_csv(self, list_input):\n",
        "      # The scraped info will be written to a CSV here.\n",
        "      try:\n",
        "          with open(self.filename, encoding='utf-8-sig', mode=\"a\", newline=\"\") as fopen:  # Open the csv file.\n",
        "              csv_writer = csv.writer(fopen)\n",
        "              csv_writer.writerow(list_input)\n",
        "      except:\n",
        "          return False"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install googletrans==4.0.0-rc1 q\n",
        "from googletrans import Translator"
      ],
      "metadata": {
        "id": "VbsklJ06pmOo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to translate text with exception handling\n",
        "def translate_text(text, source_lang, target_lang='en'):\n",
        "    translator = Translator()\n",
        "    try:\n",
        "        translation = translator.translate(text, src=source_lang, dest=target_lang)\n",
        "        return translation.text\n",
        "    except Exception as e:\n",
        "        print(f\"Translation failed for text: {text}. Error: {e}\")\n",
        "        return None\n"
      ],
      "metadata": {
        "id": "lqR8XkoUpkgk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGC5YvSz0O5f"
      },
      "source": [
        "Single scraper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mpSJ8aAdzd3r"
      },
      "outputs": [],
      "source": [
        "\n",
        "class SingleScraper :\n",
        "  #scrape single job post\n",
        "  def __init__(self, url, fields) :\n",
        "    self.url = str(url)\n",
        "    self.page = requests.get(self.url)\n",
        "    if(self.page.status_code == 200) :\n",
        "      self.soup = bs(self.page.content, \"html.parser\")\n",
        "      self.row = []\n",
        "      self.fields = fields\n",
        "    else:\n",
        "      print('Failed to retrieve the web page. Status code:', self.page.status_code)\n",
        "\n",
        "\n",
        "\n",
        "  def scrape(self) :\n",
        "\n",
        "    # The About of the offre (it takes both the offer divs those eli contains all the infos about the job post to use them after that)\n",
        "    top_annonce = self.soup.find('div', attrs={'class' : 'top-annonce'})\n",
        "    detail_offre = self.soup.find('div', attrs={'class' : re.compile(r'detail-offre(\\snotfeatured)?')})\n",
        "\n",
        "    #Start addding the about section of the job accessing first div[class=\"top-annonce\"] > h1\n",
        "    Poste = top_annonce.h1.text\n",
        "    print(\"Emploi : \" , Poste)\n",
        "    self.row.append(translate_text(Poste,'fr'))\n",
        "    # self.row.append(Classify().classify(Poste))\n",
        "\n",
        "\n",
        "    # looking for the all the il in a list div[class=\"top-annonce\"] > ul > il (eli houmma 3 )\n",
        "    identity = top_annonce.ul.find_all('li')\n",
        "\n",
        "    #company\n",
        "    print(\"Entreprise   : \", identity[0].text)\n",
        "    self.row.append(identity[0].text)\n",
        "\n",
        "    #location\n",
        "    print(\"Location  : \", identity[1].text)\n",
        "    self.row.append(identity[1].text)\n",
        "\n",
        "    #posted at\n",
        "    print(\"Date de poste : \", identity[2].text)\n",
        "    # self.row.append(self.toHour(identity[2].text.split()))\n",
        "    self.row.append(identity[2].text)\n",
        "\n",
        "    #candidates in a try a catch if there s a limit in for how many are needed\n",
        "    candidates = top_annonce.find('div', attrs={'class' : 'applicants-num'})\n",
        "    try:\n",
        "      print(\"candidats : \", candidates.text)\n",
        "      if candidates.text == '':\n",
        "        self.row.append(0)\n",
        "      else:\n",
        "        self.row.append(candidates.text)\n",
        "    except:\n",
        "      # if its empty the it feel it with \"none\"\n",
        "      print(\"candidats : 0\")\n",
        "      self.row.append(0)\n",
        "\n",
        "    # look for div[class=\"infos_job_details\"] which is inside of div[class=\"detail-offre\"]\n",
        "    detail = detail_offre.find('div', attrs={'class' : 'infos_job_details'})\n",
        "\n",
        "    #Retrieve all the divs inside of it\n",
        "    details = detail.find_all('div')\n",
        "    last = 0\n",
        "    for i in range(len(details)-1) :\n",
        "      label = (details[i].dl.dt.text).split()[0]\n",
        "      value = details[i].dl.dd.text\n",
        "      for j in range(last, len(self.fields)) :\n",
        "        if self.fields[j] == label :\n",
        "          print(label , ' : ', value)\n",
        "          # if i == 0: self.row.append(value[0])\n",
        "          # else : self.row.append(value)\n",
        "          self.row.append(value)\n",
        "          last = j+1\n",
        "          break\n",
        "        self.row.append('None')\n",
        "        print(self.fields[j] , ' : ', None)\n",
        "\n",
        "    for k in range(last, len(self.fields)):\n",
        "      print(self.fields[k] , ' : None')\n",
        "      self.row.append('None')\n",
        "\n",
        "    print(\"*********************************************************************\")\n",
        "    return self.row\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWLAy91j0Uva"
      },
      "source": [
        "global scraper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RSluMBOAzUh5"
      },
      "outputs": [],
      "source": [
        "class GlobalScraper:\n",
        "    def __init__(self, url, pageNum, filename, header):\n",
        "      self.url = str(url)\n",
        "      self.page = requests.get(self.url)\n",
        "      if(self.page.status_code == 200) :\n",
        "        self.pageNum = int(pageNum)\n",
        "        self.filename = filename\n",
        "        self.CsvWriter = Writer(filename)\n",
        "        # self.CsvWriter.write_to_csv(header)\n",
        "        self.soup = bs(self.page.content, \"html.parser\")\n",
        "        self.prefixPageUrl = self.soup.find('span', class_='pad_right_small').a['href']\n",
        "        self.prefixPageUrl = self.prefixPageUrl[:-1]\n",
        "        self.fields = [\"Postes\", \"Type\", \"Experience\", \"Niveau\", \"Rémunération\", \"Langue\", \"Genre\"] # for single scrapper\n",
        "      else:\n",
        "        print('Failed to retrieve the web page. Status code:', self.page.status_code)\n",
        "\n",
        "    def scrape(self, num):\n",
        "        if num <= self.pageNum:\n",
        "            pageUrl = str(self.url) + str(self.prefixPageUrl) + str(num)\n",
        "            print(pageUrl)\n",
        "            self.page = requests.get(pageUrl)\n",
        "            if self.page.status_code == 200:\n",
        "                self.soup = bs(self.page.content, \"html.parser\")\n",
        "                posts = self.soup.find_all('article', class_=\"media\")\n",
        "                for post in posts:\n",
        "                    postUrl = post.find('a')['href']\n",
        "                    row = SingleScraper(postUrl, self.fields).scrape()\n",
        "                    self.CsvWriter.write_to_csv(row)\n",
        "            else:\n",
        "                print('Failed to retrieve the web page. Status code:', self.page.status_code)\n",
        "\n",
        "            self.scrape(num + 1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGIKUbvt0Yg8"
      },
      "source": [
        "Execution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_cG75xrf8pVJ"
      },
      "outputs": [],
      "source": [
        "URL = \"https://www.tanitjobs.com/jobs/\"\n",
        "MAXPAGESNUM = 1\n",
        "FILENAME = 'finalData.csv'\n",
        "HEADER = [\"Position\", \"Entreprise\", \"Location\", \"Date\", \"Candidates\", \"Postes\", \"Type\", \"Experience\", \"Niveau\", \"Rémunération\", \"Langue\", \"Genre\"]\n",
        "\n",
        "#running test\n",
        "execute = GlobalScraper(URL, MAXPAGESNUM, FILENAME, HEADER)\n",
        "execute.scrape(1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Unitest**"
      ],
      "metadata": {
        "id": "kPwT3NlixNg_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import unittest\n",
        "from unittest.mock import MagicMock, patch\n",
        "\n",
        "# Define your SingleScraper class and other necessary code here\n",
        "\n",
        "class TestSingleScraper(unittest.TestCase):\n",
        "\n",
        "    def setUp(self):\n",
        "        # Set up common test data or mock objects\n",
        "        self.url = \"https://www.tanitjobs.com/jobs/?searchId=1705315644.0337&action=search&page=1\"\n",
        "        self.fields = [\"Postes\", \"Type\", \"Experience\", \"Niveau\", \"Rémunération\", \"Langue\", \"Genre\"]\n",
        "        self.header = [\"Position\", \"Entreprise\", \"Location\", \"Date\", \"Candidates\"] + self.fields\n",
        "        self.scraper = SingleScraper(self.url, self.fields)\n",
        "\n",
        "    def test_scrape_specific_url(self):\n",
        "        # Test the scrape method with a specific URL\n",
        "        with patch('requests.get') as mock_get:\n",
        "            # Mock the response for the specific URL\n",
        "            mock_get.return_value.status_code = 200\n",
        "            mock_get.return_value.content = b'<html>YourMockedHTMLContent</html>'\n",
        "\n",
        "            # Create a BeautifulSoup mock object\n",
        "            mock_soup = MagicMock()\n",
        "            with patch('bs4.BeautifulSoup', return_value=mock_soup):\n",
        "                # Call the scrape method\n",
        "                result = self.scraper.scrape()\n",
        "\n",
        "                # Add assertions based on the expected behavior of the scrape method\n",
        "                self.assertIsNotNone(result)\n",
        "                self.assertEqual(len(result), len(self.header))\n",
        "                # Add more assertions based on your specific case\n",
        "\n",
        "# Run the tests\n",
        "unittest.TextTestRunner().run(unittest.TestLoader().loadTestsFromTestCase(TestSingleScraper))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vt5HDNBZwi16",
        "outputId": "3863d177-f420-49ec-d472-af9ab016824e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "E\n",
            "======================================================================\n",
            "ERROR: test_scrape_specific_url (__main__.TestSingleScraper)\n",
            "----------------------------------------------------------------------\n",
            "Traceback (most recent call last):\n",
            "  File \"<ipython-input-12-3cf984c7391f>\", line 26, in test_scrape_specific_url\n",
            "    result = self.scraper.scrape()\n",
            "  File \"<ipython-input-5-2edebe83cbdd>\", line 22, in scrape\n",
            "    Poste = top_annonce.h1.text\n",
            "AttributeError: 'NoneType' object has no attribute 'h1'\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Ran 1 test in 6.664s\n",
            "\n",
            "FAILED (errors=1)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<unittest.runner.TextTestResult run=1 errors=1 failures=0>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}